{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac7f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model vllm/openai/gpt-oss-20b via Llama Stack at http://localhost:8321\n",
      "\n",
      "User: What's the weather like in San Francisco?\n",
      "\n",
      "Response ID: resp_f64a2890-426c-44a8-9c4c-8c11af2353c6\n",
      "Output items: 1\n",
      "  - Type: function_call\n",
      "    Function: get_weather\n",
      "    Arguments: {\"location\":\"San Francisco\"}\n",
      "    Call ID: chatcmpl-tool-b91fb03ecf523867\n",
      "    Result: {'location': 'San Francisco', 'temperature': 53, 'unit': 'fahrenheit', 'condition': 'Sunny', 'humidity': 77, 'wind_mph': 7, 'feels_like': 51}\n",
      "\n",
      "============================================================\n",
      "Response from GPT-OSS-20b with vLLM Current Main (01/14/2026)\n",
      "============================================================\n",
      "**San Francisco** (Fahrenheit)\n",
      "\n",
      "- **Temperature:** 53 °F  \n",
      "- **Feels like:** 51 °F  \n",
      "- **Condition:** Sunny  \n",
      "- **Humidity:** 77 %  \n",
      "- **Wind:** 7 mph  \n",
      "\n",
      "Enjoy the lovely weather!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Function to get weather data\n",
    "def get_weather(location: str, unit: str = \"fahrenheit\") -> dict:    \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        url = f\"https://wttr.in/{location}?format=j1\"\n",
    "        resp = requests.get(url, timeout=100)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        \n",
    "        current = data[\"current_condition\"][0]\n",
    "        temp_f = int(current[\"temp_F\"])\n",
    "        temp_c = int(current[\"temp_C\"])\n",
    "        \n",
    "        return {\n",
    "            \"location\": location,\n",
    "            \"temperature\": temp_c if unit == \"celsius\" else temp_f,\n",
    "            \"unit\": unit,\n",
    "            \"condition\": current[\"weatherDesc\"][0][\"value\"],\n",
    "            \"humidity\": int(current[\"humidity\"]),\n",
    "            \"wind_mph\": int(current[\"windspeedMiles\"]),\n",
    "            \"feels_like\": int(current[\"FeelsLikeC\"]) if unit == \"celsius\" else int(current[\"FeelsLikeF\"])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"location\": location, \"error\": str(e)}\n",
    "\n",
    "# Define the weather tool\n",
    "WEATHER_TOOL = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get the current weather for a given location\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city name, e.g. San Francisco\"\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                \"description\": \"The temperature unit to use\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prompt to get weather for San Francisco\n",
    "PROMPT = \"What's the weather like in San Francisco?\"\n",
    "\n",
    "# Agentic response from response API\n",
    "def main():\n",
    "\n",
    "    llama_stack_url = os.getenv(\"LLAMA_STACK_URL\")\n",
    "    if llama_stack_url is None:\n",
    "        raise ValueError(\"LLAMA_STACK_URL is not set in the environment variables\")\n",
    "\n",
    "    model = \"vllm/openai/gpt-oss-20b\"\n",
    "    \n",
    "    print(f\"Using model {model} via Llama Stack at {llama_stack_url}\")\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=f\"{llama_stack_url}/v1\",\n",
    "        api_key=\"none\"\n",
    "    )\n",
    "\n",
    "    # First call - model may request function calls\n",
    "    print(f\"\\nUser: {PROMPT}\")\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        tools=[WEATHER_TOOL],\n",
    "        input=PROMPT\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResponse ID: {response.id}\")\n",
    "    print(f\"Output items: {len(response.output)}\")\n",
    "    \n",
    "    # Check for function calls in the response output\n",
    "    for item in response.output:\n",
    "        print(f\"  - Type: {item.type}\")\n",
    "        if item.type == \"function_call\":\n",
    "            print(f\"    Function: {item.name}\")\n",
    "            print(f\"    Arguments: {item.arguments}\")\n",
    "            print(f\"    Call ID: {item.call_id}\")\n",
    "            \n",
    "            # Execute the function\n",
    "            args = json.loads(item.arguments)\n",
    "            result = get_weather(**args)\n",
    "            print(f\"    Result: {result}\")\n",
    "            \n",
    "            # Send the function result back\n",
    "            response = client.responses.create(\n",
    "                model=model,\n",
    "                tools=[WEATHER_TOOL],\n",
    "                previous_response_id=response.id,\n",
    "                input=[{\n",
    "                    \"type\": \"function_call_output\",\n",
    "                    \"call_id\": item.call_id,\n",
    "                    \"output\": json.dumps(result)\n",
    "                }]\n",
    "            )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Response from GPT-OSS-20b with vLLM Current Main (01/14/2026)\")\n",
    "    print(\"=\"*60)\n",
    "    print(response.output_text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
