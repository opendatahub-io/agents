{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd77dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model vllm/openai/gpt-oss-120b via Llama Stack at http://localhost:8321\n",
      "\n",
      "User: What's the weather like in San Francisco?\n",
      "\n",
      "Response ID: chatcmpl-8f43f3ae8cf4e641\n",
      "Tool calls: 1\n",
      "  - Type: function_call\n",
      "    Function: get_weather\n",
      "    Arguments: {\"location\": \"San Francisco\", \"unit\": \"fahrenheit\"}\n",
      "    Call ID: chatcmpl-tool-8b032d11005dd982\n",
      "    Result: {'location': 'San Francisco', 'temperature': 52, 'unit': 'fahrenheit', 'condition': 'Mist', 'humidity': 97, 'wind_mph': 8, 'feels_like': 49}\n",
      "\n",
      "============================================================\n",
      "Response from GPT-OSS-120b via Chat Completions API (01/15/2026)\n",
      "============================================================\n",
      "In San Francisco right now it’s **52 °F** with a misty, overcast feel. The humidity is high at about 97 %, the wind is blowing around 8 mph, and it feels a little cooler—around **49 °F**.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Function to get weather data\n",
    "def get_weather(location: str, unit: str = \"fahrenheit\") -> dict:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        url = f\"https://wttr.in/{location}?format=j1\"\n",
    "        resp = requests.get(url, timeout=100)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        \n",
    "        current = data[\"current_condition\"][0]\n",
    "        temp_f = int(current[\"temp_F\"])\n",
    "        temp_c = int(current[\"temp_C\"])\n",
    "        \n",
    "        return {\n",
    "            \"location\": location,\n",
    "            \"temperature\": temp_c if unit == \"celsius\" else temp_f,\n",
    "            \"unit\": unit,\n",
    "            \"condition\": current[\"weatherDesc\"][0][\"value\"],\n",
    "            \"humidity\": int(current[\"humidity\"]),\n",
    "            \"wind_mph\": int(current[\"windspeedMiles\"]),\n",
    "            \"feels_like\": int(current[\"FeelsLikeC\"]) if unit == \"celsius\" else int(current[\"FeelsLikeF\"])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"location\": location, \"error\": str(e)}\n",
    "\n",
    "# Define the weather tool\n",
    "WEATHER_TOOL = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get the current weather for a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city name, e.g. San Francisco\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prompt to get weather for San Francisco\n",
    "PROMPT = \"What's the weather like in San Francisco?\"\n",
    "\n",
    "# Function tool calling via Chat Completions API\n",
    "def main():\n",
    "\n",
    "    llama_stack_url = os.getenv(\"LLAMA_STACK_URL\")\n",
    "    if llama_stack_url is None:\n",
    "        raise ValueError(\"LLAMA_STACK_URL is not set in the environment variables\")\n",
    "\n",
    "    model = \"vllm/openai/gpt-oss-120b\"\n",
    "    \n",
    "    print(f\"Using model {model} via Llama Stack at {llama_stack_url}\")\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=f\"{llama_stack_url}/v1\",\n",
    "        api_key=\"none\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": PROMPT}]\n",
    "\n",
    "    # First call - model may request tool calls\n",
    "    print(f\"\\nUser: {PROMPT}\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=[WEATHER_TOOL],\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "\n",
    "    assistant_message = response.choices[0].message\n",
    "\n",
    "    print(f\"\\nResponse ID: {response.id}\")\n",
    "    print(f\"Tool calls: {len(assistant_message.tool_calls) if assistant_message.tool_calls else 0}\")\n",
    "    \n",
    "    # Check for tool calls in the response\n",
    "    if assistant_message.tool_calls:\n",
    "        messages.append(assistant_message)\n",
    "        \n",
    "        for tool_call in assistant_message.tool_calls:\n",
    "            print(f\"  - Type: function_call\")\n",
    "            print(f\"    Function: {tool_call.function.name}\")\n",
    "            print(f\"    Arguments: {tool_call.function.arguments}\")\n",
    "            print(f\"    Call ID: {tool_call.id}\")\n",
    "            \n",
    "            # Execute the function\n",
    "            args = json.loads(tool_call.function.arguments)\n",
    "            result = get_weather(**args)\n",
    "            print(f\"    Result: {result}\")\n",
    "            \n",
    "            # Add tool result to messages\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": json.dumps(result)\n",
    "            })\n",
    "        \n",
    "        # Second call - get final response with tool results\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=[WEATHER_TOOL]\n",
    "        )\n",
    "        final_response = response.choices[0].message.content\n",
    "    else:\n",
    "        final_response = assistant_message.content\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Response from GPT-OSS-120b via Chat Completions API (01/15/2026)\")\n",
    "    print(\"=\"*60)\n",
    "    print(final_response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
