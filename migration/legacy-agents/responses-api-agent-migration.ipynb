{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294e8591",
   "metadata": {},
   "source": [
    "# Migrating from Agent objects to Responses in Llama Stack\n",
    "\n",
    "Llama Stack had a set of server APIs for creating and managing the components of an agent including endpoints for grouping tools, making an agent that uses those groups of tools, making a session for that agent, and making a turn for that session.  However, these server APIs are deprecated in Llama Stack 0.3.0.  The functionality in those server APIs continue to exist the client APIs, but they are relatively limited and are not expected to undergo substantial improvement so they are not recommended for new development.  Instead, the recommended replacement for those APIs is the OpenAI-compatible Responses API which provides an all-in-one API for doing agentic reasoning.\n",
    "\n",
    "To learn more about the Responses API and why you might want to use it see [Your agent, your rules: A deep dive into the Responses API with Llama Stack](https://developers.redhat.com/articles/2025/08/20/your-agent-your-rules-deep-dive-responses-api-llama-stack) from Red Hat and/or [Why we built the Responses API](https://developers.openai.com/blog/responses-api/).  This notebook is more specifically focused on the following topic:\n",
    "\n",
    "> *What do I do if I have an existing system implemented using the old Llama Stack agent APIs and I want to use the Llama Stack Responses API instead?*\n",
    "\n",
    "To that end, we present the following:\n",
    "\n",
    "- *Getting Started*: Instructions for setting up, configuring, creating a client, and testing it out.\n",
    "- *Legacy Agent API Example*: Code that creates and runs an agent using the Llama Stack client Agent APIs.\n",
    "- *Equivalent Responses API Example*: Code that does the same thing by calling the Responses API to show how you would rewrite the first example to use Responses API.  That is probably the best approach for most users who are using the old APIs and want to move to the new one.  That code is significantly simpler and more elegant than the equivalent code using the old APIs.\n",
    "- *Emulating the Legacy Agent API*: Code that provides a wrapper around the Responses API that emulates the old agent APIs along with code that mirrors the original example using that emulation.  Note that the emulation is a partial/incomplete emulation of the old APIs that is just powerful enough to implement the simple example.  It is meant to be a starting point for you to emulate those parts of the old agent APIs that are essential for your application.\n",
    "- *Adopting a simpler agent class*: A simpler wrapper that is less consistent with the old APIs but might be an easier starting point if complete compatibility with the old APIs are less essential.\n",
    "- *Human-in-the-loop tool approval*: Example of how to have an agent that checks with the user for approval before invoking a tool.\n",
    "- *Model safety*: Example of how safety models are used with both the old agent APIs and the new Responses API.\n",
    "- *ReAct agents*: Example of using the ReAct agent construct in the Llama Stack Python client library and how to accomplish something similar using the Responses API.\n",
    "- *Multi-process architectures*: Thoughts on how to migrate applications that run different aspects of the old APIs in different processes (e.g., one process to make agents and another to consume them).\n",
    "\n",
    "\n",
    "The development of this notebook was assisted by Google Gemini and Cursor using Claude 4 Sonnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fcb7e6",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Before getting started, follow the following steps.\n",
    "\n",
    "First install Llama Stack and all of the other dependencies for this notebook.\n",
    "One way to do that is:\n",
    "\n",
    "- First install Python 3.12 or later (do not try this with older versions of Python: it will not work).\n",
    "- Then make a Python virtual environment.\n",
    "- Then within that virtual environment run:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Once everything is installed, run the Llama Stack server:\n",
    "\n",
    "```\n",
    "llama stack run run.yaml --image-type venv\n",
    "```\n",
    "\n",
    "Also run the National Parks Service Model Context Protocol (MCP) server as described in [README_NPS.md](https://github.com/The-AI-Alliance/llama-stack-examples/blob/main/notebooks/01-responses/README_NPS.md).  Download it from that location and then run it using:\n",
    "\n",
    "```\n",
    "python nps_mcp_server.py --transport sse --port 3005\n",
    "```\n",
    "\n",
    "Alternatively, you can use this notebook with some other MCP server, but then you will need to change the example query and the server details to match "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16539c5",
   "metadata": {},
   "source": [
    "Here we point to the locations of the servers we just started up above.  Also, we provide the model ID for the model we want to use.  The model ID should be one that that is specified in [run.yaml](./run.yaml).  In the [run.yaml](./run.yaml) included here, we have the following models defined:\n",
    "\n",
    "- `openai/gpt-3.5-turbo` and `openai/gpt-4o` are models from OpenAI.  They will only work if you have OPENAI_API_KEY set in your environment to a [valid OpenAI API key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key).\n",
    "- `llama-openai-compat/Llama-3.3-70B-Instruct` is a model from Meta Llama.  This will only work if you have LLAMA_API_KEY set in your environment to a valid API key for the hosted [Llama API](https://www.llama.com/products/llama-api/).\n",
    "- `watsonx/Llama-3.3-70B-Instruct` is the same model running on watsonx.ai (which has a somewhat different style for model IDs).  This will only work if you have the WATSONX_API_KEY and WATSONX_PROJECT_ID environment variables set to valid values (which requires an IBM Cloud account).  You may also need to set WATSONX_BASE_URL set if your watsonx.ai instance is running anywhere other than US South (which is the default).  Note that the watsonx provider in Llama Stack was [not working](https://github.com/llamastack/llama-stack/issues/3165) when this notebook was created, but hopefully it will work by the time you read this.\n",
    "\n",
    "If you can't or don't want to get any of those API keys, you can update [run.yaml](./run.yaml) to use [another inference provider](https://llama-stack.readthedocs.io/en/latest/providers/inference/index.html#overview).  Llama Stack includes numerous providers for calling hosted models like the ones above.  It also includes providers to call models that you deploy and run yourself using a model serving capability, e.g., the [vLLM provider](https://llama-stack.readthedocs.io/en/latest/providers/inference/remote_vllm.html) or the [ollama provider](https://llama-stack.readthedocs.io/en/latest/providers/inference/remote_ollama.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_STACK_URL = \"http://localhost:8321/\"\n",
    "LLAMA_STACK_MODEL_IDS = [\n",
    "    \"openai/gpt-3.5-turbo\",\n",
    "    \"openai/gpt-4o\",\n",
    "    \"llama-openai-compat/Llama-3.3-70B-Instruct\",\n",
    "    \"watsonx/Llama-3.3-70B-Instruct\"\n",
    "]\n",
    "\n",
    "# Using gpt-4o for this demo, but feel free to try one of the others or add more to run.yaml.\n",
    "LLAMA_STACK_MODEL_ID = LLAMA_STACK_MODEL_IDS[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317f80e",
   "metadata": {},
   "source": [
    "Replace these with other values if you are using a different MCP server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47947a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPS_MCP_URL = \"http://localhost:3005/sse/\"\n",
    "NPS_EXAMPLE_PROMPT = \"Tell me about some parks in Rhode Island, and let me know if there are any upcoming events at them.\"\n",
    "NPS_EXAMPLE_FOLLOWUP_PROMPT = \"Which of these is happening the soonest?\"\n",
    "\n",
    "NPS_INSTRUCTIONS = \"You are a helpful assistant that can answer questions about the National Parks Service.\"\n",
    "\n",
    "# The NPS MCP server does not require an access token, but some MCP servers do.\n",
    "# We are sending it a dummy token here to show how to send the access token to the MCP server.\n",
    "NPS_ACCESS_TOKEN = \"frog\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb41df0",
   "metadata": {},
   "source": [
    "Next we instantiate the client:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12945dc9",
   "metadata": {},
   "source": [
    "Then we test to see if it is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e7fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion_response = client.chat.completions.create(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date\n",
    "\n",
    "def pretty_print(obj) -> None:\n",
    "    \"\"\"\n",
    "    Recursively prints an object's __dict__ in a nicely formatted JSON.\n",
    "    Handles nested objects and lists of objects.\n",
    "    \"\"\"\n",
    "    def recursive_serializer(o):\n",
    "        if hasattr(o, '__dict__'):\n",
    "            return o.__dict__\n",
    "        # Handle types that are not directly JSON serializable\n",
    "        if isinstance(o, date):\n",
    "            return o.isoformat()\n",
    "        # For other types, raise a TypeError to let the default handler fail.\n",
    "        raise TypeError(f\"Object of type {o.__class__.__name__} is not JSON serializable\")\n",
    "\n",
    "    # Determine what to serialize\n",
    "    data_to_serialize = obj.__dict__ if hasattr(obj, \"__dict__\") else obj\n",
    "\n",
    "    print(json.dumps(\n",
    "        data_to_serialize,\n",
    "        indent=2,\n",
    "        default=recursive_serializer\n",
    "    ))\n",
    "\n",
    "\n",
    "pretty_print(chat_completion_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c183da",
   "metadata": {},
   "source": [
    "## Legacy Agent API Example\n",
    "\n",
    "Here is a simple example for how to use the Llama Stack Agent API.  We don't recommend this approach because the primary strategic focus for Llama Stack is OpenAI-compatible APIs since those reflect a de facto industry \"standard\".  We're showing that API here for context to motivate the alternative approaches in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73eefa",
   "metadata": {},
   "source": [
    "Here (commented out) is an example of how an Agent was intended to be instantiated in older versions of Llama Stack as per some [old documentation for MCP with Agent in Llama Stack](https://llamastack.github.io/v0.2.22/building_applications/tools.html#model-context-protocol-mcp):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b40b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_stack_client import Agent\n",
    "# import uuid\n",
    "\n",
    "# client.toolgroups.register(\n",
    "#     toolgroup_id=\"mcp::nps\",\n",
    "#     provider_id=\"model-context-protocol\",\n",
    "#     mcp_endpoint=URL(uri=NPS_MCP_URL),\n",
    "# )\n",
    "\n",
    "# agent = Agent(\n",
    "#     model=LLAMA_STACK_MODEL_ID,\n",
    "#     instructions=NPS_INSTRUCTIONS,\n",
    "#     client=client,\n",
    "#     tools=[\"mcp::nps\"],\n",
    "#     extra_headers={\n",
    "#         \"X-LlamaStack-Provider-Data\": json.dumps(\n",
    "#             {\n",
    "#                 \"mcp_headers\": {\n",
    "#                     NPS_MCP_URL: {\n",
    "#                         \"Authorization\": f\"Bearer {NPS_ACCESS_TOKEN}\",\n",
    "#                     },\n",
    "#                 },\n",
    "#             }\n",
    "#         ),\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508848e",
   "metadata": {},
   "source": [
    "The approach above no longer works, but you can now specify MCP tools directly in the Agent object in the Python client for similar behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03541dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent\n",
    "import uuid\n",
    "\n",
    "agent = Agent(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    instructions=NPS_INSTRUCTIONS,\n",
    "    client=client,\n",
    "    tools=[{\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": NPS_MCP_URL,\n",
    "            \"server_label\": \"National Parks Service tools\",\n",
    "            \"headers\": {\n",
    "                \"Authorization\": f\"Bearer {NPS_ACCESS_TOKEN}\",\n",
    "            },\n",
    "        }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique session ID for the example.\n",
    "# This is not required, but it is useful to have if you want to create multiple sessions without restarting Llama Stack.\n",
    "session_id = agent.create_session(f\"nps_session-{uuid.uuid4().hex}\")\n",
    "\n",
    "agent_response1 = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": NPS_EXAMPLE_PROMPT}],\n",
    "    session_id=session_id,\n",
    "    stream=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e668205",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(agent_response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10155e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_response2 = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": NPS_EXAMPLE_FOLLOWUP_PROMPT}],\n",
    "    session_id=session_id,\n",
    "    stream=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae54bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(agent_response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9545693",
   "metadata": {},
   "source": [
    "As you can see this approach still works and can be a viable substitute for the approach commented out above.  It is a bit simpler and more elegant since you no longer need to separately register tool groups and then use them in your agent definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59587b",
   "metadata": {},
   "source": [
    "## Equivalent Responses API Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6b0f8",
   "metadata": {},
   "source": [
    "Here is some code using the Responses API that is roughly equivalent to the Agent example above.  As you can see, it no longer has separate calls for:\n",
    "\n",
    "- Registering tools\n",
    "- Creating an agent\n",
    "- Creating a session\n",
    "- Issuing a query for that session\n",
    "\n",
    "The Responses API takes the list of tools as an argument so there is no need to pre-register the tools.  It does not make a persistent agent object but it does make a session object implicitly within the call -- you can see how that is used in the second call, in which `previous_response_id=responses_api_response1.id` is set to indicate that the second call is a continuation of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da76153",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_api_response1 = client.responses.create(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    input=NPS_EXAMPLE_PROMPT,\n",
    "    instructions=NPS_INSTRUCTIONS,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": NPS_MCP_URL,\n",
    "            \"server_label\": \"National Parks Service tools\",\n",
    "            \"headers\": {\n",
    "                \"Authorization\": f\"Bearer {NPS_ACCESS_TOKEN}\",\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3963a755",
   "metadata": {},
   "source": [
    "Here we print the entire object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(responses_api_response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8179c71",
   "metadata": {},
   "source": [
    "However, for some applications, we really only care about the actual text.  Here is how to print that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response_text(response):\n",
    "    for output_block in response.output:\n",
    "        if output_block.type == \"message\":\n",
    "            for content_block in output_block.content:\n",
    "                if hasattr(content_block, \"text\"):\n",
    "                    print(content_block.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response_text(responses_api_response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_api_response2 = client.responses.create(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    input=NPS_EXAMPLE_FOLLOWUP_PROMPT,\n",
    "    instructions=NPS_INSTRUCTIONS,\n",
    "    previous_response_id=responses_api_response1.id,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": NPS_MCP_URL,\n",
    "            \"server_label\": \"National Parks Service tools\",\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response_text(responses_api_response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8dccb5",
   "metadata": {},
   "source": [
    "## Emulating the Legacy Agent API\n",
    "\n",
    "For most use cases, the approach above is probably the best way to migrate from the Agent API to the Responses API.  However, if you have a large amount of code that uses the old API, you might want to have some objects that emulate the old Agent API instead (the approach in the commented out example at the top of the previous session).  Here is a very simple example of how to do that.\n",
    "\n",
    "The simple example only covers the functionality needed in the original simple example above.  We'll get into some advanced functionality in later sections, but there are also many parameters in the legacy API that we do not provide sample code for.  We don't recommend trying to build a complete implementation of every parameter value in the entire API.  Instead, you can use this example as a starting point and then fill in those parameters and/or values that are important to your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = {}\n",
    "SESSIONS = {}\n",
    "\n",
    "class LegacyURL:\n",
    "    def __init__(self, uri):\n",
    "        self.uri = uri\n",
    "\n",
    "def toolgroups_register(toolgroup_id, provider_id, mcp_endpoint=None):\n",
    "    \"\"\"\n",
    "    This replaces the client.toolgroups.register() call in the legacy agent API.  Since this version manages the\n",
    "    tool group registration internally, it does not need to use the client object.\n",
    "    \"\"\"\n",
    "    if provider_id == \"model-context-protocol\":\n",
    "        TOOLS[toolgroup_id] = {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": mcp_endpoint.uri,\n",
    "            \"server_label\": toolgroup_id,\n",
    "        }\n",
    "    else:\n",
    "        # TODO: Add support for other providers, not needed for this example.\n",
    "        raise ValueError(f\"Unsupported provider: {provider_id}\")\n",
    "\n",
    "def convert_response_to_legacy_agent_response(response):\n",
    "    \"\"\"\n",
    "    This is a placeholder for now.  The code to convert objects to the legacy agent response format would go here.\n",
    "    Note that just returning the response object is good enough for this example because the response object has\n",
    "    all of the same fields that we are using in the example print statements.  However, in a more complex application\n",
    "    you may need to do more work here.\n",
    "    \"\"\"\n",
    "    return response\n",
    "\n",
    "class LegacyAgent:\n",
    "    def __init__(self, model, instructions, client, tools, extra_headers):\n",
    "        self.model = model\n",
    "        self.instructions = instructions\n",
    "        self.client = client\n",
    "        self.tool_ids = tools\n",
    "\n",
    "        header_json = extra_headers[\"X-LlamaStack-Provider-Data\"]\n",
    "        headers = json.loads(header_json)\n",
    "        self.headers = headers[\"mcp_headers\"]\n",
    "    \n",
    "    def create_session(self, session_id):\n",
    "        SESSIONS[session_id] = []\n",
    "        return session_id\n",
    "    \n",
    "    def create_turn(self, messages, session_id, stream=False):\n",
    "        # Note that the stream parameter is not used.  That works for this example because we are not using the stream=True,\n",
    "        # but if you are using that option, you will need to update this code.\n",
    "        if session_id not in SESSIONS:\n",
    "            raise ValueError(f\"Session {session_id} not found\")\n",
    "            \n",
    "        previous_response_id = None\n",
    "        if len(SESSIONS[session_id]) > 0:\n",
    "            previous_response_id = SESSIONS[session_id][-1].id\n",
    "\n",
    "        tools = [TOOLS[tool_id] for tool_id in self.tool_ids]\n",
    "        for tool in tools:\n",
    "            if tool[\"server_url\"] in self.headers:\n",
    "                tool[\"headers\"] = self.headers[tool[\"server_url\"]]\n",
    "\n",
    "        response = client.responses.create(\n",
    "            model=self.model,\n",
    "            # TODO: This is using the last message in the list, which is fine for this example, but\n",
    "            # if you call create_turn() with multiple messages, you will need to update this code.\n",
    "            input=messages[-1][\"content\"],\n",
    "            previous_response_id=previous_response_id,\n",
    "            instructions=self.instructions,\n",
    "            tools=tools\n",
    "        )\n",
    "        SESSIONS[session_id].append(response)\n",
    "        return convert_response_to_legacy_agent_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e695106",
   "metadata": {},
   "source": [
    "With this simple wrapper in place, we can maintain much of the structure of the original Agent code we saw earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolgroups_register(\n",
    "    toolgroup_id=\"mcp::nps\",\n",
    "    provider_id=\"model-context-protocol\",\n",
    "    mcp_endpoint=LegacyURL(uri=NPS_MCP_URL),\n",
    ")\n",
    "\n",
    "agent = LegacyAgent(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    instructions=NPS_INSTRUCTIONS,\n",
    "    client=client,\n",
    "    tools=[\"mcp::nps\"],\n",
    "    extra_headers={\n",
    "        \"X-LlamaStack-Provider-Data\": json.dumps(\n",
    "            {\n",
    "                \"mcp_headers\": {\n",
    "                    NPS_MCP_URL: {\n",
    "                        \"Authorization\": f\"Bearer {NPS_ACCESS_TOKEN}\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601210",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = agent.create_session(f\"nps_session-{uuid.uuid4().hex}\")\n",
    "\n",
    "agent_response1 = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": NPS_EXAMPLE_PROMPT}],\n",
    "    session_id=session_id,\n",
    "    stream=False,\n",
    ")\n",
    "print_response_text(agent_response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52260cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_response2 = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": NPS_EXAMPLE_FOLLOWUP_PROMPT}],\n",
    "    session_id=session_id,\n",
    "    stream=False,\n",
    ")\n",
    "print_response_text(agent_response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192988a",
   "metadata": {},
   "source": [
    "## Adopting a simpler agent class\n",
    "\n",
    "As you can see above, a downside of the LegacyAgent is it drags in the clunky inelegance of the legacy APIs (by design).  Calling Responses API directly as shown in the section before that is often the best way to avoid this inelegance, but some developers really want a more object-oriented experience with an object representing an agent especially if they have existing code that is structured around such an object.  The example below is a compromise between calling a mirror of the legacy Agent class (as shown in the previous section) and just calling Responses directly (as shown in the section before that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea75b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleExampleSession:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.previous_response_id = None\n",
    "    \n",
    "    def create_turn(self, input):\n",
    "        response = self.agent.client.responses.create(\n",
    "            model=self.agent.model,\n",
    "            input=input,\n",
    "            previous_response_id=self.previous_response_id,\n",
    "            instructions=self.agent.instructions,\n",
    "            tools=self.agent.tools\n",
    "        )\n",
    "        self.previous_response_id = response.id\n",
    "        return response\n",
    "\n",
    "class SimpleExampleAgent:\n",
    "    def __init__(self, model, instructions, client, tools):\n",
    "        self.model = model\n",
    "        self.instructions = instructions\n",
    "        self.client = client\n",
    "        self.tools = tools\n",
    "\n",
    "    def create_session(self):\n",
    "        return SimpleExampleSession(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52acefd1",
   "metadata": {},
   "source": [
    "As you can see, this is much simpler than LegacyAgent and the code to use it is simpler too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45209ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_agent = SimpleExampleAgent(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    instructions=NPS_INSTRUCTIONS,\n",
    "    client=client,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": NPS_MCP_URL,\n",
    "            \"server_label\": \"National Parks Service tools\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "simple_session = simple_agent.create_session()\n",
    "\n",
    "simple_agent_response1 = simple_session.create_turn(NPS_EXAMPLE_PROMPT)\n",
    "print_response_text(simple_agent_response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88473ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_agent_response2 = simple_session.create_turn(NPS_EXAMPLE_FOLLOWUP_PROMPT)\n",
    "print_response_text(simple_agent_response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b5ebb",
   "metadata": {},
   "source": [
    "Of course, the downside of the simpler version here is that it is less compatible with the legacy API so if you have existing code that uses the legacy API, it will need more rewriting to use this structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508746e0",
   "metadata": {},
   "source": [
    "## Human-in-the-loop tool approval\n",
    "\n",
    "Both LegacyAgent and SimpleExampleAgent here are extremely minimal and that there are many features of Responses that are not covered in these example class.  In the next section, we will address one of those features mainly to show how you can go about expanding on the basic examples.  A more full-featured agent construct is outside the scope of this notebook; it is up to the reader to explore the [Responses API documentation](https://platform.openai.com/docs/api-reference/responses) and decide which ones fit their use case.  We will use the SimpleExampleAgent as a starting point because it is simpler to build on, but you could also make the same kinds of changes to LegacyAgent if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a13cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleExampleSessionWithApproval:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.previous_response_id = None\n",
    "    \n",
    "    def create_turn(self, input):\n",
    "        response = self.agent.client.responses.create(\n",
    "            model=self.agent.model,\n",
    "            input=input,\n",
    "            previous_response_id=self.previous_response_id,\n",
    "            instructions=self.agent.instructions,\n",
    "            tools=self.agent.tools\n",
    "        )\n",
    "        self.previous_response_id = response.id\n",
    "\n",
    "        approval_requests = []\n",
    "        for block in response.output:\n",
    "            if block.type == \"mcp_approval_request\":\n",
    "                approval_request = (block.name, block.arguments, block.id)\n",
    "                approval_requests.append(approval_request)\n",
    "\n",
    "        return response, approval_requests if approval_requests else None\n",
    "\n",
    "    def create_approval_turn(self, approval_requests_to_approve):\n",
    "        \"\"\"\n",
    "        This is a helper method to create a turn that is used to approve or reject tool calls.\n",
    "        The input is a dictionary that maps approval requests to boolean values indicating whether to approve or reject.\n",
    "        \"\"\"\n",
    "        approval_request_entries = []\n",
    "        for approval_request in approval_requests_to_approve:\n",
    "            approval_request_entries.append({\n",
    "                \"type\": \"mcp_approval_request\",\n",
    "                \"approval_request_id\": approval_request[2],\n",
    "                \"approval\": approval_requests_to_approve[approval_request]\n",
    "            })\n",
    "\n",
    "        response = self.client.responses.create(\n",
    "            model=self.model,\n",
    "            input=approval_request_entries,\n",
    "            previous_response_id=self.previous_response_id,\n",
    "            instructions=self.instructions,\n",
    "            tools=self.tools\n",
    "        )\n",
    "        self.previous_response_id = response.id\n",
    "        return response\n",
    "    \n",
    "class SimpleExampleAgentWithApproval:\n",
    "    def __init__(self, model, instructions, client, tools):\n",
    "        self.model = model\n",
    "        self.instructions = instructions\n",
    "        self.client = client\n",
    "        self.tools = tools\n",
    "\n",
    "    def create_session(self):\n",
    "        return SimpleExampleSessionWithApproval(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a97a3",
   "metadata": {},
   "source": [
    "Unlike the version in the previous section, this version it checks to see if there is an `mcp_approval_request` in the response, which there may be if you set `\"requires_approval\": True` in one of your MCP tool entries as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_agent_with_approval = SimpleExampleAgentWithApproval(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    instructions=NPS_INSTRUCTIONS,\n",
    "    client=client,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": NPS_MCP_URL,\n",
    "            \"server_label\": \"National Parks Service tools\",\n",
    "            \"requires_approval\": True\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae5652",
   "metadata": {},
   "source": [
    "The `create_turn` method on a session now returns both the response object and optionally an approval request extracted from that response object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d9bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_session_with_approval = simple_agent_with_approval.create_session()\n",
    "\n",
    "simple_agent_response1, approval_requests1 = simple_session_with_approval.create_turn(NPS_EXAMPLE_PROMPT)\n",
    "print_response_text(simple_agent_response1)\n",
    "print(approval_requests1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed8218",
   "metadata": {},
   "source": [
    "When the client gets an approval request, it should ask a user or apply whatever other logic it deems appropriate to approve or reject the request to run the tool.  This can be very important if you have tools that perform destructive operations since models will sometimes choose the wrong tools or the wrong values for those tools.  Here is an example of how to approve a tool use with this implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60009bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_approval_from_user(approval_request):\n",
    "    \"\"\"\n",
    "    This is a placeholder for the actual logic to get approval from the user.\n",
    "    It prints the approval request so you can see the information that would be\n",
    "    available to the user.  In a real application, you would replace this with\n",
    "    the actual logic to show this to the user in a reasonable way and then ask\n",
    "    for a yes/no decision on whether to approve the tool call.\n",
    "    \"\"\"\n",
    "    print(approval_request)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67649104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approve_tool_use(approval_requests):\n",
    "    \"\"\"\n",
    "    Method to iterate over the approval requests, get approval boolean from the user for each one, and then send the approval booleans to the session.\n",
    "    Each approval boolean is a boolean value indicating whether to approve or reject the tool call.\n",
    "    The approval requests are a list of tuples, each containing the tool name, tool arguments, and approval request id.\n",
    "    \"\"\"\n",
    "    approval_requests_to_approve = {}\n",
    "    for approval_request in approval_requests:\n",
    "        approval_bool = get_approval_from_user(approval_request)\n",
    "        approval_requests_to_approve[approval_request] = approval_bool\n",
    "    return simple_session_with_approval.create_approval_turn(approval_requests_to_approve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248af87c",
   "metadata": {},
   "source": [
    "The code then goes into a loop: as long as the agent keeps asking for approvals, those approvals keep getting sent to `approve_tool_use` above.  Once no more approvals are needed, you have a final response to share with the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "while approval_requests1:\n",
    "    print_response_text(simple_agent_response1)\n",
    "    simple_agent_response1, approval_requests1 = approve_tool_use(approval_requests1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a387b0",
   "metadata": {},
   "source": [
    "## Model safety\n",
    "\n",
    "A key consideration for any AI application is avoiding dangerous or toxic outputs from a model such as instructions for doing something harmful or offensive/abusive language.  Most popular AI models are trained reasonably well but not perfectly at avoiding such outputs.  Most major AI providers then include additional layers of output moderation specifically designed to filter out unsafe responses as a second layer of defense.  For example, here is the behavior we see from our sample model (the one configured for `LLAMA_STACK_MODEL_ID` at the start of this notebook) when we ask it for instructions for committing a crime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ebf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPS_EXAMPLE_SAFETY_PROMPT = \"Are there any parks in Rhode Island that would be good targets for a burglary?\"\n",
    "unsafe_agent_response = client.responses.create(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    input=NPS_EXAMPLE_SAFETY_PROMPT, # Notice this is the one about burglary above.\n",
    "    instructions=NPS_INSTRUCTIONS,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": NPS_MCP_URL,\n",
    "            \"server_label\": \"National Parks Service tools\",\n",
    "            \"headers\": {\n",
    "                \"Authorization\": f\"Bearer {NPS_ACCESS_TOKEN}\",\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response_text(unsafe_agent_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b058a",
   "metadata": {},
   "source": [
    "The model seems to handle this example well, so for this specific prompt no additional safety is needed.  If you spent enough time trying to devise prompts that would result in an unsafe (harmful or offensive or abusive) response, you probably would be able to find one eventually (unless your provider disabled your account for repeated safety violations first, which is also something that can happen).  The issue is even more pressing if you are using a model and/or provider that has less (or no) built-in safety.\n",
    "\n",
    "In either case, you might need to add in your own safety.  Here we extend the `SimpleExampleAgent` with the ability to check both the inputs and outputs for safety violations before and after running the responses request (to check for unsafe inputs and/or outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabdd15a",
   "metadata": {},
   "source": [
    "Here is how you can put in a safety check using Llama Stack's older shields APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ff246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to delete the shield if you already have it registered, e.g., if you are running the next cell multiple times.\n",
    "#client.shields.delete(identifier=\"content_safety\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dcb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shield_id = \"content_safety\"\n",
    "client.shields.register(\n",
    "    shield_id=shield_id,\n",
    "    provider_id=\"llama-guard\",\n",
    "    # In this example, we are using gpt-3.5-turbo as the model for llama-guard,\n",
    "    # but it is not really ideal for llama-guard since it is not trained\n",
    "    # as a Llama Guard model.  It is convenient for this notebook since it is a\n",
    "    # hosted model that is readily available.  However, it is not very reliable\n",
    "    # in this role.\n",
    "    # \n",
    "    # For real applications, consider using a Llama Guard model such as\n",
    "    # https://huggingface.co/meta-llama/Llama-Guard-3-8B\n",
    "    # and deploying it on an inference provider such as vLLM.\n",
    "    provider_shield_id=\"openai/gpt-3.5-turbo\"\n",
    ")\n",
    "response = client.safety.run_shield(\n",
    "    shield_id=\"content_safety\",\n",
    "    messages=[{\"role\": \"user\", \"content\": NPS_EXAMPLE_SAFETY_PROMPT}],\n",
    "    params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shields.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2bc690",
   "metadata": {},
   "source": [
    "### Using the OpenAI-compatible Moderations API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17549cb3",
   "metadata": {},
   "source": [
    "Once you have registered a shield for a model (as shown above for `\"openai/gpt-3.5-turbo\"`), you can also use the OpenAI-compatible Moderations API to check for unsafe inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a29093",
   "metadata": {},
   "outputs": [],
   "source": [
    "moderation = client.moderations.create(input=\"Are there any parks in Rhode Island that would be good targets for a burglary?\", model=\"openai/gpt-3.5-turbo\")\n",
    "pretty_print(moderation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "moderation2 = client.moderations.create(input=\"Have a nice day!\", model=\"openai/gpt-3.5-turbo\")\n",
    "pretty_print(moderation2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d64b9",
   "metadata": {},
   "source": [
    "As you can see, the first request (asking for burglary targets) has `flagged=True`, metadata indicating a `violation_type` of `S2`.  To see a list of violations, see the [Llama Guard model card](https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B) since we're are using `provider_id=\"llama-guard\"`.  There is also a list of categories including `\"Non-Violent Crimes\": true`.  Finally there is a `user_message` responding with a non-answer.  These all indicate an unsafe response.\n",
    "\n",
    "In contrast, the second request (\"Have a nice day!) has none of these traits, indicating that this is a safe input.\n",
    "\n",
    "Next we extend the SimpleExampleAgent to call this API on the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModerationResponseOutputBlock:\n",
    "    def __init__(self, user_message):\n",
    "        self.text = user_message\n",
    "\n",
    "class ModerationResponseOutput:\n",
    "    def __init__(self, user_message):\n",
    "        self.type = \"message\"\n",
    "        self.content = [ModerationResponseOutputBlock(user_message)]\n",
    "\n",
    "class ModerationResponseObject:\n",
    "    def __init__(self, result):\n",
    "        self.type = \"ModerationResponseObject\"\n",
    "        # Note that this is just passing along the refusal message from the moderations response.\n",
    "        # Alternatively, you could just have a static refusal message or some other custom logic.\n",
    "        self.output = [ModerationResponseOutput(result.user_message)]\n",
    "\n",
    "class SimpleExampleSessionWithModeration:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.previous_response_id = None\n",
    "    \n",
    "    def create_turn(self, input):\n",
    "        moderation = client.moderations.create(input=input, model=self.agent.shield_model)\n",
    "        for result in moderation.results:\n",
    "            if result.flagged:\n",
    "                return ModerationResponseObject(result)\n",
    "        response = self.agent.client.responses.create(\n",
    "            model=self.agent.model,\n",
    "            input=input,\n",
    "            previous_response_id=self.previous_response_id,\n",
    "            instructions=self.agent.instructions,\n",
    "            tools=self.agent.tools\n",
    "        )\n",
    "        # Note that you might also want to iterate over the blocks in the response\n",
    "        # and check for unsafe outputs in each of them or all of them together.\n",
    "        # In this example, we just check the input, but some applications might want\n",
    "        # to check the output instead or to check both.\n",
    "        self.previous_response_id = response.id\n",
    "        return response\n",
    "\n",
    "class SimpleExampleAgentWithModeration:\n",
    "    def __init__(self, model, shield_model, instructions, client, tools):\n",
    "        self.model = model\n",
    "        self.shield_model = shield_model\n",
    "        self.instructions = instructions\n",
    "        self.client = client\n",
    "        self.tools = tools\n",
    "        shields = client.shields.list()\n",
    "        has_shield = any(shield.provider_resource_id == shield_model for shield in shields)\n",
    "        if not has_shield:\n",
    "            shield_id = f\"content_safety_{shield_model}\"\n",
    "            client.shields.register(\n",
    "                shield_id=shield_id,\n",
    "                provider_id=\"llama-guard\",\n",
    "                provider_shield_id=shield_model\n",
    "            )\n",
    "\n",
    "    def create_session(self):\n",
    "        return SimpleExampleSessionWithModeration(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0cfe8b",
   "metadata": {},
   "source": [
    "We verify that this still works with the safe original example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_agent_with_moderation = SimpleExampleAgentWithModeration(\n",
    "    model=LLAMA_STACK_MODEL_ID, # gpt-4o\n",
    "    shield_model=\"openai/gpt-3.5-turbo\",\n",
    "    instructions=NPS_INSTRUCTIONS,\n",
    "    client=client,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": NPS_MCP_URL,\n",
    "            \"server_label\": \"National Parks Service tools\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "simple_session_with_moderation = simple_agent_with_moderation.create_session()\n",
    "\n",
    "simple_agent_response_with_moderation1 = simple_session_with_moderation.create_turn(NPS_EXAMPLE_PROMPT)\n",
    "print_response_text(simple_agent_response_with_moderation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5a25e",
   "metadata": {},
   "source": [
    "Next we try with the NPS_EXAMPLE_SAFETY_PROMPT (the one about burglary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e764a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_agent_response_with_moderation2 = simple_session_with_moderation.create_turn(NPS_EXAMPLE_SAFETY_PROMPT)\n",
    "print_response_text(simple_agent_response_with_moderation2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28898c0c",
   "metadata": {},
   "source": [
    "The output here is our simple ModerationResponseObject with just the moderation output as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11330501",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(simple_agent_response_with_moderation2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa20303",
   "metadata": {},
   "source": [
    "Notice that this example didn't include the human-in-the-loop tool approval capabilities discussed in the previous section.  You may want to combine the extensions in both of these sections and then continue to extend with all of the other advanced features you need for your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8eb0d9",
   "metadata": {},
   "source": [
    "### Using extra_body/guardrails\n",
    "\n",
    "Llama Stack also has a special non-breaking extension to the OpenAI-compatible APIs.  This lets you embed the moderation call inside the Responses API so you don't need to call it separately. To do this, you use a parameter called `extra_body` and provide a dictionary with key `guardrails` and value of a list of shield `identifier` values.  Here is an example of use (assuming you have already run `client.shields.register` as we did earlier in this notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2422a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "shields = client.shields.list()\n",
    "first_shield_identifier = shields[0].identifier\n",
    "shields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1ecdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsafe_agent_response_using_guardrails = client.responses.create(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    input=NPS_EXAMPLE_SAFETY_PROMPT, # This is the one about burglary above.\n",
    "    instructions=NPS_INSTRUCTIONS,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": NPS_MCP_URL,\n",
    "            \"server_label\": \"National Parks Service tools\",\n",
    "            \"headers\": {\n",
    "                \"Authorization\": f\"Bearer {NPS_ACCESS_TOKEN}\",\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    extra_body={\"guardrails\": [first_shield_identifier]}, # This is the special extension to the OpenAI-compatible APIs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d582c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsafe_agent_response_using_guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e55f8b",
   "metadata": {},
   "source": [
    "If you look closely at the ResponseObject above, you will see that the first entry in the `output.content` list is a dictionary with key `type` and value `refusal`.  Compare that to a \"normal\" responses output below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_response = client.responses.create(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    input=\"What is the capital of France?\",\n",
    ")\n",
    "normal_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c4240",
   "metadata": {},
   "source": [
    "Here you see that the first entry in the `output.content` list is a `OutputOpenAIResponseMessageContentUnionMember2` object with a `type` *field* with value `output_text`.  This complicates processing of output blocks because some of them have a `type` field and others have a dictionary with a `type` key.  This is a consequence of the fact that the `extra_body`/`guardrails` capability is a non-breaking extension to the OpenAI APIs but the actual response object classes in the Llama Stack client only cover outputs produced by the actual OpenAI APIs, so the client represents this extension information as a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98721ea9",
   "metadata": {},
   "source": [
    "An advantage of the extra_body/guardrails approach is that you don't need to separately call the Moderations API before and/or after the call to Responses because it is all done for you in a single call.  One disadvantage of this approach is the added complexity in the client data structures discussed above.  Another disadvantage of course is that you no longer have access to the fine-grained control that you get from being able to make separate Moderations calls before and/or after Responses, inspect all the structured output and use it in your application.  For example, some chat UI applications might want to use different icons or colors for input moderation violations, output moderation violations, violations of different types, etc.  However, if you don't mind dealing with the data structures and you don't need all that fine-grained control, then the `extra_body` approach might be the ideal fit.  Here is how we do that in our simple example agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlternateModerationResponseOutputBlock:\n",
    "    def __init__(self, user_message):\n",
    "        self.text = user_message\n",
    "\n",
    "class AlternateModerationResponseOutput:\n",
    "    def __init__(self, user_message):\n",
    "        self.type = \"message\"\n",
    "        self.content = [AlternateModerationResponseOutputBlock(user_message)]\n",
    "\n",
    "class AlternateModerationResponseObject:\n",
    "    def __init__(self, user_message):\n",
    "        self.type = \"AlternateModerationResponseObject\"\n",
    "        self.output = [AlternateModerationResponseOutput(user_message)]\n",
    "\n",
    "class AlternateSimpleExampleSessionWithModeration:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.previous_response_id = None\n",
    "    \n",
    "    def create_turn(self, input):\n",
    "        response = self.agent.client.responses.create(\n",
    "            model=self.agent.model,\n",
    "            input=input,\n",
    "            previous_response_id=self.previous_response_id,\n",
    "            instructions=self.agent.instructions,\n",
    "            tools=self.agent.tools,\n",
    "            extra_body={\"guardrails\": [self.agent.shield_identifier]}\n",
    "        )\n",
    "        self.previous_response_id = response.id\n",
    "        for output_block in response.output:\n",
    "            if output_block.type == \"message\":\n",
    "                for content_block in output_block.content:\n",
    "                    # We need to check for a dictionary with a `type` key because the extra_body/guardrails\n",
    "                    # feature returns a dictionary but other content blocks are generally objects with a `type` field.\n",
    "                    if isinstance(content_block, dict) and content_block[\"type\"] == \"refusal\":\n",
    "                        # Note that this is just passing along the refusal message from the guardrails.\n",
    "                        # Alternatively, you could just have a static refusal message or some other custom logic.\n",
    "                        # Or you could just return the response object as is and let the caller decide what to do.\n",
    "                        return AlternateModerationResponseObject(content_block[\"refusal\"])\n",
    "        return response\n",
    "\n",
    "class AlternateSimpleExampleAgentWithModeration:\n",
    "    def __init__(self, model, shield_model, instructions, client, tools):\n",
    "        self.model = model\n",
    "        self.instructions = instructions\n",
    "        self.client = client\n",
    "        self.tools = tools\n",
    "        shields = client.shields.list()\n",
    "        self.shield_identifier = None\n",
    "        for shield in shields:\n",
    "            if shield.provider_resource_id == shield_model:\n",
    "                self.shield_identifier = shield.identifier\n",
    "        if not self.shield_identifier:\n",
    "            shield_id = f\"content_safety_{shield_model}\"\n",
    "            client.shields.register(\n",
    "                shield_id=shield_id,\n",
    "                provider_id=\"llama-guard\",\n",
    "                provider_shield_id=shield_model\n",
    "            )\n",
    "            self.shield_identifier = shield_id\n",
    "\n",
    "    def create_session(self):\n",
    "        return AlternateSimpleExampleSessionWithModeration(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e45bf3",
   "metadata": {},
   "source": [
    "Once again, we first instantiate the agent and verify that a safe prompt still works as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shields.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_simple_agent_with_moderation = AlternateSimpleExampleAgentWithModeration(\n",
    "    model=LLAMA_STACK_MODEL_ID, # gpt-4o\n",
    "    shield_model=\"openai/gpt-3.5-turbo\",\n",
    "    instructions=NPS_INSTRUCTIONS,\n",
    "    client=client,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": NPS_MCP_URL,\n",
    "            \"server_label\": \"National Parks Service tools\",\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35532254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alt_simple_session_with_moderation = alt_simple_agent_with_moderation.create_session()\n",
    "alt_simple_agent_response_with_moderation1 = alt_simple_session_with_moderation.create_turn(\"Hello!\")\n",
    "print_response_text(alt_simple_agent_response_with_moderation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd06da1",
   "metadata": {},
   "source": [
    "Then we test with the unsafe (burglary) example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_simple_agent_response_with_moderation2 = alt_simple_session_with_moderation.create_turn(NPS_EXAMPLE_SAFETY_PROMPT)\n",
    "print_response_text(alt_simple_agent_response_with_moderation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(alt_simple_agent_response_with_moderation2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfa0c4",
   "metadata": {},
   "source": [
    "Notice that this is returning our custom \"AlternateModerationResponseObject\" because the moderation flagged this output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031395e",
   "metadata": {},
   "source": [
    "\n",
    "This notebook has presented two alternative approaches to using guardrails models:\n",
    "\n",
    "- Calling the Moderations API on the inputs and outputs of the Responses call lets you control the guardrails yourself at the cost of having to make additional API calls.\n",
    "- Including `extra_body={\"guardrails\": [shield_id]}` in the Responses call gets Llama Stack to run the guardrails for you at the cost of less control and some added complexity in the client data structures.\n",
    "\n",
    "Developers should consider the advantages and disadvantages of each option and choose the one that is best suited to their application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46128c",
   "metadata": {},
   "source": [
    "## ReAct Agents\n",
    "\n",
    "The Llama Stack Python Client has a client-side [ReAct Agent](https://github.com/llamastack/llama-stack-client-python/blob/main/src/llama_stack_client/lib/agents/react/agent.py) construct.  This construct still works.  It operates by listing the tools and formulating a text prompt that combines the user request, system instructions, and tool list into one big text prompt.  Those system instructions direct the model to take in observations and output thoughts before selecting an action, which can improve accuracy because it forces the model to split up the challenging process of identifying the right tool and parameter values into multiple simpler steps.  The accuracy benefits come at a cost of longer latency and higher token generation charges since outputting thoughts takes time and tokens.  For some challenging, high-value tasks the improved accuracy is worth the added time and cost.  For more details about ReAct see [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629).\n",
    "\n",
    "ReAct was a revolutionary technique in 2022 when the paper was first published, but it is less relevant today because the lessons from ReAct have been expanded upon and incorporated into other parts of the technology stack.  In particular, some key elements and where they have wound up are:\n",
    "\n",
    "- *Listing tools with formal input schemas*: In 2022, this needed to be done inside a single block of text because generative AI models were trained to take in a single block of text and output a single block of text.  However, modern \"chat completion\" models are generally trained to take a formal structure in which there are pre-defined separators and schemas for session history, tools, etc.  Of course, it is still *possible* to just lump all that info into a plain text blob like people did in 2022, but since the models are tuned to use the formal structure, it seems likely that you will get better results more often using that structure.  The way to use that structure is just to use the appropriate parameters in the Responses or Chat Completions APIs.  For example, if you list your tools in the `tools` parameter of Responses, a well-trained chat completion model is more likely to select the right tool more often than if you just describe them in plain text in the user message.\n",
    "- *Outputting thoughts before deciding how to ReAct*: Recent \"reasoning\" models are trained specifically to do this using a formal structure.  For example, gpt-oss uses a format called [Harmony](https://cookbook.openai.com/articles/openai-harmony) in which the models output their reasoning before selecting tools and/or providing response text to end users for exactly this reason.\n",
    "\n",
    "So if you are working with a modern \"reasoning\" model and an API that has formal structures for listing tools and outputting thoughts, then you would normally expect that your model and infrastructure are already providing the core benefits of ReAct.  On the other hand, if you are just using a \"chat completion\" model that is trained for tool selection but isn't explicitly trained to output thoughts before selecting a tool, then some sort of hybrid where you use a ReAct style prompt to encourage outputting thoughts before selecting a tool might be effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672a483",
   "metadata": {},
   "source": [
    "### How the existing ReAct agents in the Llama Stack Python client work\n",
    "\n",
    "Before getting into such a hybrid approach, let's take a look at how the ReAct agents in the Llama Stack Python client work now.  First we will register an MCP server and then list all the tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.types.toolgroup_register_params import McpEndpoint\n",
    "\n",
    "client.toolgroups.register(\n",
    "    toolgroup_id=\"mcp::nps\",\n",
    "    provider_id=\"model-context-protocol\",\n",
    "    mcp_endpoint=McpEndpoint(uri=NPS_MCP_URL),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df316f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_defs = client.tools.list(toolgroup_id=\"mcp::nps\")\n",
    "tool_defs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8008623f",
   "metadata": {},
   "source": [
    "Above are the tools from the MCP server (`search_parks`, etc.)  The ReAct prompt generator takes these tools in the form of dictionary objects, so we extract those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff8e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_def_dictionary_objects = [x.__dict__ for x in tool_defs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75033c",
   "metadata": {},
   "source": [
    "and then we call the prompt generator to produce a prompt using these tool definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.react.agent import get_default_react_instructions\n",
    "print(get_default_react_instructions(tool_def_dictionary_objects))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7ab60",
   "metadata": {},
   "source": [
    "As you can see, there are a few parts to this prompt:\n",
    "\n",
    "1. It explains the output format to produce including how to list selected tools and their parameter values.\n",
    "2. It tells the model to always take exactly one action on each turn.\n",
    "3. It provides the model with a bunch of examples of the desired output format (\"few-shot prompting\").\n",
    "4. It says \"Above example were using notional tools that might not exist for you. You only have access to these tools:\"\n",
    "5. Then it lists the actual MCP tools and the arguments that they take in a semi-formal notation.\n",
    "6. Then it lists a set of general rules such as not re-doing a tool with the same parameters.\n",
    "7. Finally, it offers the model a reward of $1,000,000 if the model succeeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb1511",
   "metadata": {},
   "source": [
    "The offer of a reward is not part of the original ReAct paper, but it became popular around the same time because there were some anecdotes about it being effective.  Late 2022 through early 2023 were an extraordinarily peculiar time in the history of AI.  It seems not inconceivable it could be effective if the training data it is exposed to includes many examples of people offering a reward for answering a question followed by extraordinarily thoughtful, high quality answers.  In that case, the model pre-training could have learned this statistical trend and thus be more likely to produce extraordinarily thoughtful, high quality answers following such a prompt.  On the other hand, there doesn't seem to be substantial evidence that this actually works and it does add to the length of the input, driving up compute costs and latency.  So it is not included in the prompt for the \"ReAct-inspired agentic reasoning for chat completions models using the Responses API\" section later in this notebook, but feel free to try adding it and see for yourself if it helps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8e255",
   "metadata": {},
   "source": [
    "### Using the existing ReAct agents in the Llama Stack Python client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d47d14",
   "metadata": {},
   "source": [
    "Before proposing an alternative, this notebook provides an example of how to use the existing ReAct agent construct in the Python client, which does still work in the current version of Llama Stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db120b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "react_agent = ReActAgent(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    instructions=NPS_INSTRUCTIONS,\n",
    "    client=client,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": NPS_MCP_URL,\n",
    "            \"server_label\": \"National Parks Service tools\",\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1479aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = agent.create_session(f\"nps_react_session-{uuid.uuid4().hex}\")\n",
    "\n",
    "agent_response1 = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": NPS_EXAMPLE_PROMPT}],\n",
    "    session_id=session_id,\n",
    "    stream=False,\n",
    ")\n",
    "print_response_text(agent_response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891998b0",
   "metadata": {},
   "source": [
    "On this simple example query, it seems to get comparable results to the results you get from calling Responses directly (see the earlier \"Equivalent Responses API Example\" section for more details).  The ReAct agent is instructing the model to output thoughts before acting which potentially makes it more effective.  On the other hand, pushing tool descriptions into the text of the prompt (instead of using the tool-list structure that the model is trained to process) potentially makes it less effective.  So either might be more effective for your use cases.  The Responses API is a core element of the Llama Stack strategy which focuses mainly on compatibility with the de facto industry \"standard\" of the OpenAI APIs.  In contrast, the ReAct agent is left over from an older strategy for Llama Stack and thus seems unlikely to undergo substantial improvement in the future.\n",
    "\n",
    "The next section proposes an alternative based on the Responses API that preserves some of the benefits of ReAct while also benefitting from some of the power of more modern chat completions models that are trained to handle structured inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e291e",
   "metadata": {},
   "source": [
    "## ReAct-style agentic reasoning for chat completions models using the Responses API\n",
    "\n",
    "If you are using a \"reasoning\" model such as gpt-oss or gpt-4o or gpt-5 or Claude Sonnet 4.5, then the models are already trained to output thoughts before selecting actions, so there isn't really a need to explicitly prompt the model to do so -- just make sure that you are enabling the reasoning mode and then everything you might have wanted from ReAct is already covered using structures that the model is specifically trained to handle.  Conversely, if you are using a model that is only pre-trained and instruction-tuned but without a formal chat completion template, then you probably want to use something like the original ReAct capability described above.  However, many models fall in between these extremes: they are trained to recognize specific structures for a chat-completions API including both session history and tool calling, but they are not explicitly trained to output reasoning traces.  For models of this sort, you want to use those API structures for chaining messages together and telling the model what tools are available.  Thus an ideal fit might be a compromise solution like the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "REACT_STYLE_SYSTEM_INSTRUCTIONS = \"\"\"\n",
    "You are an expert assistant who can solve any task using tool calls. You will be given a task to solve as best you can.\n",
    "\n",
    "You can use the result of the previous action as input for the next action.\n",
    "The observation will always be the response from calling the tool: it can represent a file, like \"image_1.jpg\". You do not need to generate them, it will be provided to you. \n",
    "Then you can use it as input for the next action. You can do it for instance as follows:\n",
    "\n",
    "Here are the rules you should always follow to solve your task:\n",
    "1. Always use the right arguments for tools. Never use variable names in tool parameters, use the value instead.\n",
    "2. Call a tool only when needed: do not call a tool if you do not need information, try to solve the task yourself.\n",
    "3. Never re-do a tool call that you previously did with the exact same parameters.\n",
    "\"\"\"\n",
    "\n",
    "REACT_STYLE_THINK_PROMPT_TEMPLATE = \"\"\"\n",
    "An AI agent has been given a request from a user.  It has access to the specified tools.\n",
    "Think about what that AI agent should do next.  That might include using a tool or it\n",
    "might involve solving the task without using a tool.  Do not output any text other than\n",
    "your thoughts about what the agent should do next.\n",
    "\n",
    "Here is the user request:\n",
    "\n",
    "{user_request}\n",
    "\"\"\"\n",
    "\n",
    "REACT_STYLE_ACT_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI agent that has been given a request from a user.  There are some existing\n",
    "thoughts for you to consider.  You should either produce a final response or select\n",
    "tools to call.\n",
    "\n",
    "Here is the user request:\n",
    "\n",
    "{user_request}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff95a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReActStyleSimpleExampleSession:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.previous_response_id = None\n",
    "    \n",
    "    def create_turn(self, input):\n",
    "        # Thinking phase\n",
    "        response = self.agent.client.responses.create(\n",
    "            model=self.agent.model,\n",
    "            input=REACT_STYLE_THINK_PROMPT_TEMPLATE.format(user_request=input),\n",
    "            previous_response_id=self.previous_response_id,\n",
    "            instructions=REACT_STYLE_SYSTEM_INSTRUCTIONS,\n",
    "            tools=self.agent.tools,\n",
    "            # In the Thinking phase, we don't want the model to output anything other than the thoughts.\n",
    "            # So it shouldn't actually select tools, it should just explain why it might or might not want to use a tool.\n",
    "\n",
    "            # This parameter is not supported in the current version of the Llama Stack,\n",
    "            # but you should uncomment it if you are using a future version that does support it.\n",
    "            #tool_choice=\"none\"\n",
    "        )\n",
    "        self.previous_response_id = response.id\n",
    "\n",
    "        # For demo purposes, we will print the response from the thinking phase.\n",
    "        print(f\"Thinking phase response:\")\n",
    "        print_response_text(response)\n",
    "\n",
    "        # Acting phase\n",
    "        response = self.agent.client.responses.create(\n",
    "            model=self.agent.model,\n",
    "            input=REACT_STYLE_ACT_PROMPT_TEMPLATE.format(user_request=input),\n",
    "            # Note that we are using the previous response id from the thinking phase\n",
    "            # as the previous response id for the acting phase.  This allows the\n",
    "            # acting phase to use the result of the thinking phase as input.\n",
    "            previous_response_id=self.previous_response_id,\n",
    "            instructions=REACT_STYLE_SYSTEM_INSTRUCTIONS,\n",
    "            tools=self.agent.tools,\n",
    "        )\n",
    "        self.previous_response_id = response.id\n",
    "        return response\n",
    "\n",
    "class ReActStyleSimpleExampleAgent:\n",
    "    def __init__(self, model, client, tools):\n",
    "        self.model = model\n",
    "        self.client = client\n",
    "        self.tools = tools\n",
    "\n",
    "    def create_session(self):\n",
    "        return ReActStyleSimpleExampleSession(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e82b80",
   "metadata": {},
   "source": [
    "Now we instantiate this agent and try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ef130",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_react_style_agent = ReActStyleSimpleExampleAgent(\n",
    "    model=LLAMA_STACK_MODEL_ID,\n",
    "    client=client,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": NPS_MCP_URL,\n",
    "            \"server_label\": \"National Parks Service tools\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "simple_react_style_session = simple_react_style_agent.create_session()\n",
    "\n",
    "simple_react_style_agent_response1 = simple_react_style_session.create_turn(NPS_EXAMPLE_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b6cf6e",
   "metadata": {},
   "source": [
    "As you can see above, the thinking phase response includes an overall plan for how to satisfy the requirement.  That plan is then available as an input for the next step.  This can be helpful to a model for the same reason it can be helpful for a human doing a challenging task to first have an abstract end-to-end plan before starting to act: coming up with an outline and filling in the details are both challenging tasks so trying to do them at the same time can be harder than doing each separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926018ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response_text(simple_react_style_agent_response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a2c6d",
   "metadata": {},
   "source": [
    "As you can see, we still get the same (correct) behavior from this example.  For more challenging examples or less powerful models, you might see higher quality (but slower and more expensive) results from this approach.\n",
    "\n",
    "As in the earlier examples, you can combine this enhancement with other enhancements (human-in-the-loop tool approval, model safety, etc.) to develop an agent object that meets your needs.  Alternatively, you can just call the Responses API in the ways shown here without having a container object. These are just some examples to help you get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa126bcf",
   "metadata": {},
   "source": [
    "## Multi-process architectures\n",
    "\n",
    "The examples above assume that the agent creation, session creation, and turn creation are all taking place in the same process.  However, some users of the legacy Agents API might be having one process that creates agents (e.g., an agent management console) and another process that uses the agents (e.g., a chatbot app that invokes that process).  Assuming these processes have different life cycles and run on different kinds of machines, here are some examples of ideas for how to handle these cases:\n",
    "\n",
    "- If the agent creation logic is simple and static, then you can just move that agent creation logic into the application that uses the agent as in the examples above.  One downside of doing that is that you need to release a new version of the application that uses the agent every time the definition of the agent changes.  If that's not feasible for you, consider one of the other options.\n",
    "- You can have some sort of key-value storage mechanism contain a description of the agent (i.e., the instructions, list of tools).  The process that creates agents can write that configuration to the storage and the process that invokes the agent can read from it.\n",
    "- You can have the process that creates the agent deploy a container with that agent into a cluster and the process that invokes the agent call that container (which would then call Llama Stack via the Responses API).  This is a much heavier and more disruptive change than just putting agent configuration into a key-value store, but potentially much more powerful too since you can build complex agents with multiple phases that use different models under different circumstances, etc.\n",
    "\n",
    "There are many other possible architectures to explore too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3e36f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3_12_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
